{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "\n",
    "from jre_utils.datapath import model_ready_data_paths, model_output_data_paths\n",
    "from jre_utils.data import JapanRETimeSeriesDataset, PadAndMask, ToNumpy, ToTensor\n",
    "from jre_utils.models import TimeSeriesTransformerModel\n",
    "from jre_utils.engine import (\n",
    "    evaluate_weighted,\n",
    "    train_weighted,\n",
    ")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_type = \"all\"\n",
    "\n",
    "metrics = {\n",
    "    \"median\": \"unit_price_median\",\n",
    "}\n",
    "\n",
    "granularity_columns = [\"area\", \"area_code\", \"asset_type\"]\n",
    "group_by_columns = granularity_columns + [\"year\"]\n",
    "display_columns = [\"unit_price\", \"total_traded_area\", \"count\"]\n",
    "\n",
    "metric_key = f\"median\"\n",
    "metric = metrics[metric_key]\n",
    "\n",
    "metric_pct_chg = metric + \"_pct_chg\"\n",
    "normalized_metric_pct_chg = metric_pct_chg + \"_normalized_yearly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    return df[~df[metric_pct_chg].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2007\n",
    "eval_start_year = 2021  # eval_years = [2021, 2022]\n",
    "eval_end_year = 2022\n",
    "\n",
    "dataset_key = \"transactions\"\n",
    "years_ahead = 2\n",
    "dataset_name = f\"sequence_{dataset_key}_{asset_type}_{metric_key}_{years_ahead}\"\n",
    "output_dataset_name = f\"{dataset_name}_{eval_start_year}\"\n",
    "model_ready_data_path = model_ready_data_paths[dataset_name]\n",
    "model_output_data_path = model_output_data_paths[output_dataset_name]\n",
    "\n",
    "df = pd.read_csv(model_ready_data_path)\n",
    "df = df[df[\"year\"] <= eval_end_year]\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df.sort_values(by=[\"year\"]).reset_index(drop=True)\n",
    "\n",
    "# Weighting by population\n",
    "df[\"log_population\"] = df[\"population\"].apply(lambda x: np.log10(1 + x))\n",
    "df[\"weight\"] = df.groupby(\"year\")[\"log_population\"].transform(lambda x: x - x.min() + 1)\n",
    "\n",
    "train_years = list(range(start_year, eval_start_year))\n",
    "eval_years = list(range(eval_start_year, eval_end_year + 1))\n",
    "\n",
    "train_dataframes = {\n",
    "    f\"{year}\": prepare_dataframe(df[df[\"year\"] == year]) for year in train_years\n",
    "}\n",
    "\n",
    "eval_dataframes = {\n",
    "    f\"{year}\": prepare_dataframe(df[df[\"year\"] == year]) for year in eval_years\n",
    "}\n",
    "\n",
    "combined_eval_df = prepare_dataframe(df[df[\"year\"] >= eval_start_year])\n",
    "\n",
    "# Finally\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_normalize_columns = [\n",
    "    metric,\n",
    "    \"count\",\n",
    "    \"total_traded_area\",\n",
    "    \"in_migrations\",\n",
    "    \"out_migrations\",\n",
    "    \"population\",\n",
    "    \"taxpayer_count\",\n",
    "    \"taxable_income\",\n",
    "    \"taxable_income_per_taxpayer\",\n",
    "    \"total_tax\",\n",
    "    \"new_dwellings\",\n",
    "    \"existing_dwellings\",\n",
    "]\n",
    "\n",
    "normalize_columns = [\n",
    "    metric_pct_chg,\n",
    "    \"yearly_price_growth\",\n",
    "    \"count_growth\",\n",
    "    \"total_tax_growth\",\n",
    "    \"taxable_income_growth\",\n",
    "    \"taxable_income_per_taxpayer_growth\",\n",
    "    \"net_migration_ratio\",\n",
    "    \"new_dwellings_ratio\",\n",
    "    \"taxpayer_count_growth\",\n",
    "]\n",
    "\n",
    "maintain_columns = [\n",
    "    metric_pct_chg,\n",
    "    \"migrations_is_available\",\n",
    "    \"taxable_income_is_available\",\n",
    "    \"dwellings_is_available\",\n",
    "    \"total_tax_is_available\",\n",
    "    \"metric_pct_chg_is_available\",\n",
    "]\n",
    "\n",
    "id_columns = [\"area_code\", \"area\", \"year\", \"asset_type\"]\n",
    "\n",
    "feature_columns = (\n",
    "    [f\"{column}_log_normalized_yearly\" for column in log_normalize_columns]\n",
    "    + [f\"{column}_normalized_yearly\" for column in normalize_columns]\n",
    "    + maintain_columns\n",
    "    + ([\"condo\", \"land\"] if asset_type == \"all\" else [])\n",
    ")\n",
    "\n",
    "final_columns = id_columns + feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = {\n",
    "    f\"{year}\": JapanRETimeSeriesDataset(\n",
    "        df,\n",
    "        train_df,\n",
    "        feature_columns=feature_columns,\n",
    "        metrics=[normalized_metric_pct_chg],\n",
    "        weight_column=\"weight\",\n",
    "        transform=transforms.Compose([ToNumpy(), PadAndMask(), ToTensor()]),\n",
    "    )\n",
    "    for year, train_df in train_dataframes.items()\n",
    "}\n",
    "\n",
    "train_dataloaders = {\n",
    "    f\"{year}\": DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    for year, train_dataset in train_datasets.items()\n",
    "}\n",
    "\n",
    "# eval_datasets = {\n",
    "#     f\"{year}\": JapanRETimeSeriesDataset(\n",
    "#         df,\n",
    "#         eval_df,\n",
    "#         feature_columns=feature_columns,\n",
    "#         metrics=[normalized_metric_pct_chg],\n",
    "#         weight_column=\"weight\",\n",
    "#         transform=transforms.Compose([ToNumpy(), PadAndMask(), ToTensor()]),\n",
    "#     )\n",
    "#     for year, eval_df in eval_dataframes.items()\n",
    "# }\n",
    "\n",
    "# eval_dataloaders = {\n",
    "#     f\"{year}\": DataLoader(\n",
    "#         eval_dataset,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         shuffle=False,\n",
    "#         num_workers=0,\n",
    "#     )\n",
    "#     for year, eval_dataset in eval_datasets.items()\n",
    "# }\n",
    "\n",
    "combined_eval_dataset = JapanRETimeSeriesDataset(\n",
    "    df,\n",
    "    combined_eval_df,\n",
    "    feature_columns=feature_columns,\n",
    "    metrics=[normalized_metric_pct_chg],\n",
    "    weight_column=\"weight\",\n",
    "    transform=transforms.Compose([ToNumpy(), PadAndMask(), ToTensor()]),\n",
    ")\n",
    "\n",
    "combined_eval_dataloader = DataLoader(\n",
    "    combined_eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(feature_columns)\n",
    "d_model = 256\n",
    "d_hid = 256\n",
    "nlayers = 8\n",
    "nhead = 8\n",
    "dropout = 0\n",
    "enc_dropout = 0\n",
    "\n",
    "model = TimeSeriesTransformerModel(\n",
    "    n_features=n_features,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    d_hid=d_hid,\n",
    "    nlayers=nlayers,\n",
    "    dropout=dropout,\n",
    "    enc_dropout=enc_dropout,\n",
    "    device=device,\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4 # 3e-4\n",
    "weight_decay = 1 # 1\n",
    "num_epochs_per_year = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      " Year: 2007\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 8.9262, Eval Loss: 13.0465\n",
      " Train R^2: -0.0639, Eval R^2: -0.6062\n",
      " Epoch: 1\n",
      " Train Loss: 18.2385, Eval Loss: 8.5232\n",
      " Train R^2: -0.8803, Eval R^2: -0.1235\n",
      " Epoch: 2\n",
      " Train Loss: 10.0409, Eval Loss: 7.4992\n",
      " Train R^2: -0.1293, Eval R^2: -0.0112\n",
      " Epoch: 3\n",
      " Train Loss: 7.2929, Eval Loss: 8.1791\n",
      " Train R^2: 0.1127, Eval R^2: -0.0808\n",
      " Epoch: 4\n",
      " Train Loss: 7.5570, Eval Loss: 8.5876\n",
      " Train R^2: 0.0802, Eval R^2: -0.1231\n",
      "-----------------\n",
      " Year: 2008\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 11.1625, Eval Loss: 8.3056\n",
      " Train R^2: -0.2208, Eval R^2: -0.1011\n",
      " Epoch: 1\n",
      " Train Loss: 6.6461, Eval Loss: 8.7891\n",
      " Train R^2: -0.0272, Eval R^2: -0.1393\n",
      " Epoch: 2\n",
      " Train Loss: 7.4777, Eval Loss: 9.1475\n",
      " Train R^2: -0.0997, Eval R^2: -0.1738\n",
      " Epoch: 3\n",
      " Train Loss: 7.4090, Eval Loss: 8.1222\n",
      " Train R^2: -0.1283, Eval R^2: -0.0649\n",
      " Epoch: 4\n",
      " Train Loss: 6.4822, Eval Loss: 7.7104\n",
      " Train R^2: -0.0108, Eval R^2: -0.0224\n",
      "-----------------\n",
      " Year: 2009\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 8.5596, Eval Loss: 7.4608\n",
      " Train R^2: -0.1045, Eval R^2: 0.0002\n",
      " Epoch: 1\n",
      " Train Loss: 7.1948, Eval Loss: 7.3492\n",
      " Train R^2: 0.0483, Eval R^2: 0.0176\n",
      " Epoch: 2\n",
      " Train Loss: 6.7564, Eval Loss: 7.2116\n",
      " Train R^2: 0.1045, Eval R^2: 0.0295\n",
      " Epoch: 3\n",
      " Train Loss: 6.4851, Eval Loss: 7.2469\n",
      " Train R^2: 0.1321, Eval R^2: 0.0251\n",
      " Epoch: 4\n",
      " Train Loss: 6.3031, Eval Loss: 7.2584\n",
      " Train R^2: 0.1550, Eval R^2: 0.0226\n",
      "-----------------\n",
      " Year: 2010\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 7.5346, Eval Loss: 6.9389\n",
      " Train R^2: 0.0358, Eval R^2: 0.0710\n",
      " Epoch: 1\n",
      " Train Loss: 6.6182, Eval Loss: 7.0642\n",
      " Train R^2: 0.1460, Eval R^2: 0.0625\n",
      " Epoch: 2\n",
      " Train Loss: 6.3394, Eval Loss: 7.3450\n",
      " Train R^2: 0.1832, Eval R^2: 0.0338\n",
      " Epoch: 3\n",
      " Train Loss: 6.0888, Eval Loss: 7.8478\n",
      " Train R^2: 0.2175, Eval R^2: -0.0258\n",
      " Epoch: 4\n",
      " Train Loss: 5.9530, Eval Loss: 6.8036\n",
      " Train R^2: 0.2329, Eval R^2: 0.0967\n",
      "-----------------\n",
      " Year: 2011\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 7.7325, Eval Loss: 7.1600\n",
      " Train R^2: 0.0616, Eval R^2: 0.0656\n",
      " Epoch: 1\n",
      " Train Loss: 6.4324, Eval Loss: 6.6136\n",
      " Train R^2: 0.2103, Eval R^2: 0.1250\n",
      " Epoch: 2\n",
      " Train Loss: 6.0204, Eval Loss: 6.2780\n",
      " Train R^2: 0.2627, Eval R^2: 0.1655\n",
      " Epoch: 3\n",
      " Train Loss: 5.7614, Eval Loss: 6.3597\n",
      " Train R^2: 0.2960, Eval R^2: 0.1536\n",
      " Epoch: 4\n",
      " Train Loss: 5.5416, Eval Loss: 6.4956\n",
      " Train R^2: 0.3238, Eval R^2: 0.1361\n",
      "-----------------\n",
      " Year: 2012\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 6.8966, Eval Loss: 6.2478\n",
      " Train R^2: 0.1046, Eval R^2: 0.1633\n",
      " Epoch: 1\n",
      " Train Loss: 5.8137, Eval Loss: 5.9732\n",
      " Train R^2: 0.2477, Eval R^2: 0.2037\n",
      " Epoch: 2\n",
      " Train Loss: 5.2740, Eval Loss: 5.8958\n",
      " Train R^2: 0.3148, Eval R^2: 0.2110\n",
      " Epoch: 3\n",
      " Train Loss: 4.9496, Eval Loss: 5.7219\n",
      " Train R^2: 0.3574, Eval R^2: 0.2316\n",
      " Epoch: 4\n",
      " Train Loss: 4.8424, Eval Loss: 5.7740\n",
      " Train R^2: 0.3713, Eval R^2: 0.2243\n",
      "-----------------\n",
      " Year: 2013\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 6.3244, Eval Loss: 5.8996\n",
      " Train R^2: 0.1868, Eval R^2: 0.2114\n",
      " Epoch: 1\n",
      " Train Loss: 5.2190, Eval Loss: 5.7408\n",
      " Train R^2: 0.3360, Eval R^2: 0.2306\n",
      " Epoch: 2\n",
      " Train Loss: 4.9042, Eval Loss: 5.4376\n",
      " Train R^2: 0.3716, Eval R^2: 0.2693\n",
      " Epoch: 3\n",
      " Train Loss: 4.6605, Eval Loss: 5.3537\n",
      " Train R^2: 0.4022, Eval R^2: 0.2815\n",
      " Epoch: 4\n",
      " Train Loss: 4.5714, Eval Loss: 5.5180\n",
      " Train R^2: 0.4137, Eval R^2: 0.2620\n",
      "-----------------\n",
      " Year: 2014\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 6.5525, Eval Loss: 5.7658\n",
      " Train R^2: 0.1812, Eval R^2: 0.2312\n",
      " Epoch: 1\n",
      " Train Loss: 5.8455, Eval Loss: 5.1352\n",
      " Train R^2: 0.2594, Eval R^2: 0.3162\n",
      " Epoch: 2\n",
      " Train Loss: 5.2570, Eval Loss: 5.4064\n",
      " Train R^2: 0.3332, Eval R^2: 0.2754\n",
      " Epoch: 3\n",
      " Train Loss: 5.0766, Eval Loss: 5.4513\n",
      " Train R^2: 0.3573, Eval R^2: 0.2679\n",
      " Epoch: 4\n",
      " Train Loss: 4.9890, Eval Loss: 5.1312\n",
      " Train R^2: 0.3676, Eval R^2: 0.3090\n",
      "-----------------\n",
      " Year: 2015\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 6.2696, Eval Loss: 6.5002\n",
      " Train R^2: 0.2150, Eval R^2: 0.1598\n",
      " Epoch: 1\n",
      " Train Loss: 5.2228, Eval Loss: 5.2313\n",
      " Train R^2: 0.3424, Eval R^2: 0.3090\n",
      " Epoch: 2\n",
      " Train Loss: 5.1223, Eval Loss: 5.2637\n",
      " Train R^2: 0.3451, Eval R^2: 0.3030\n",
      " Epoch: 3\n",
      " Train Loss: 4.5720, Eval Loss: 5.0915\n",
      " Train R^2: 0.4221, Eval R^2: 0.3237\n",
      " Epoch: 4\n",
      " Train Loss: 4.4519, Eval Loss: 5.2710\n",
      " Train R^2: 0.4370, Eval R^2: 0.3009\n",
      "-----------------\n",
      " Year: 2016\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 5.7256, Eval Loss: 5.8517\n",
      " Train R^2: 0.2536, Eval R^2: 0.2384\n",
      " Epoch: 1\n",
      " Train Loss: 4.5685, Eval Loss: 5.8608\n",
      " Train R^2: 0.4050, Eval R^2: 0.2229\n",
      " Epoch: 2\n",
      " Train Loss: 4.4836, Eval Loss: 4.9398\n",
      " Train R^2: 0.4142, Eval R^2: 0.3409\n",
      " Epoch: 3\n",
      " Train Loss: 4.2411, Eval Loss: 4.8627\n",
      " Train R^2: 0.4487, Eval R^2: 0.3494\n",
      " Epoch: 4\n",
      " Train Loss: 4.0458, Eval Loss: 4.8770\n",
      " Train R^2: 0.4738, Eval R^2: 0.3475\n",
      "-----------------\n",
      " Year: 2017\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 5.0762, Eval Loss: 4.7197\n",
      " Train R^2: 0.3390, Eval R^2: 0.3667\n",
      " Epoch: 1\n",
      " Train Loss: 4.5112, Eval Loss: 5.0974\n",
      " Train R^2: 0.4121, Eval R^2: 0.3176\n",
      " Epoch: 2\n",
      " Train Loss: 4.3110, Eval Loss: 4.7173\n",
      " Train R^2: 0.4381, Eval R^2: 0.3664\n",
      " Epoch: 3\n",
      " Train Loss: 4.1468, Eval Loss: 4.8096\n",
      " Train R^2: 0.4586, Eval R^2: 0.3537\n",
      " Epoch: 4\n",
      " Train Loss: 4.0571, Eval Loss: 4.8489\n",
      " Train R^2: 0.4713, Eval R^2: 0.3486\n",
      "-----------------\n",
      " Year: 2018\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 6.2048, Eval Loss: 5.2939\n",
      " Train R^2: 0.1801, Eval R^2: 0.2993\n",
      " Epoch: 1\n",
      " Train Loss: 5.2663, Eval Loss: 5.0430\n",
      " Train R^2: 0.2959, Eval R^2: 0.3286\n",
      " Epoch: 2\n",
      " Train Loss: 4.8291, Eval Loss: 5.1872\n",
      " Train R^2: 0.3538, Eval R^2: 0.3070\n",
      " Epoch: 3\n",
      " Train Loss: 4.6566, Eval Loss: 4.8166\n",
      " Train R^2: 0.3758, Eval R^2: 0.3539\n",
      " Epoch: 4\n",
      " Train Loss: 4.5569, Eval Loss: 4.9138\n",
      " Train R^2: 0.3884, Eval R^2: 0.3404\n",
      "-----------------\n",
      " Year: 2019\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 6.0232, Eval Loss: 4.9161\n",
      " Train R^2: 0.1906, Eval R^2: 0.3418\n",
      " Epoch: 1\n",
      " Train Loss: 5.2875, Eval Loss: 4.8385\n",
      " Train R^2: 0.2936, Eval R^2: 0.3505\n",
      " Epoch: 2\n",
      " Train Loss: 4.7288, Eval Loss: 5.4230\n",
      " Train R^2: 0.3744, Eval R^2: 0.2804\n",
      " Epoch: 3\n",
      " Train Loss: 4.4816, Eval Loss: 5.4988\n",
      " Train R^2: 0.4051, Eval R^2: 0.2701\n",
      " Epoch: 4\n",
      " Train Loss: 4.2798, Eval Loss: 5.3213\n",
      " Train R^2: 0.4326, Eval R^2: 0.2884\n",
      "-----------------\n",
      " Year: 2020\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 6.6199, Eval Loss: 5.6326\n",
      " Train R^2: 0.1197, Eval R^2: 0.2397\n",
      " Epoch: 1\n",
      " Train Loss: 5.2691, Eval Loss: 4.7894\n",
      " Train R^2: 0.2982, Eval R^2: 0.3530\n",
      " Epoch: 2\n",
      " Train Loss: 4.8859, Eval Loss: 4.7463\n",
      " Train R^2: 0.3507, Eval R^2: 0.3673\n",
      " Epoch: 3\n",
      " Train Loss: 4.6643, Eval Loss: 4.8077\n",
      " Train R^2: 0.3813, Eval R^2: 0.3607\n",
      " Epoch: 4\n",
      " Train Loss: 4.5632, Eval Loss: 4.7691\n",
      " Train R^2: 0.3952, Eval R^2: 0.3647\n"
     ]
    }
   ],
   "source": [
    "# Incremental training and evaluation\n",
    "progress_bar = None\n",
    "\n",
    "train_losses, train_r2_scores = [], []\n",
    "eval_losses, eval_r2_scores = [], []\n",
    "\n",
    "yearly_metrics = {\n",
    "    year: {\n",
    "        \"train_losses\": [],\n",
    "        \"train_r2_scores\": [],\n",
    "        \"eval_losses\": [],\n",
    "        \"eval_r2_scores\": [],\n",
    "    }\n",
    "    for year in train_years\n",
    "}\n",
    "\n",
    "for year, train_dataloader in train_dataloaders.items():\n",
    "    print(f\"-----------------\")\n",
    "    print(f\" Year: {year}\")\n",
    "    print(f\"-----------------\")\n",
    "\n",
    "    num_training_steps = num_epochs_per_year * len(train_dataloader)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",  # constant\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs_per_year):\n",
    "        train_loss, train_r2_score = train_weighted(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            progress_bar,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_r2_scores.append(train_r2_score)\n",
    "\n",
    "        eval_loss, eval_r2_score = evaluate_weighted(\n",
    "            model, combined_eval_dataloader, device=device\n",
    "        )\n",
    "        eval_losses.append(eval_loss)\n",
    "        eval_r2_scores.append(eval_r2_score)\n",
    "\n",
    "        print(f\" Epoch: {epoch}\")\n",
    "        print(f\" Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "        print(f\" Train R^2: {train_r2_score:.4f}, Eval R^2: {eval_r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in train_years:\n",
    "    train_losses = yearly_metrics[year][\"train_losses\"]\n",
    "    eval_losses = yearly_metrics[year][\"eval_losses\"]\n",
    "\n",
    "    plt.plot(train_r2_scores, label = \"train\")\n",
    "    plt.plot(eval_r2_scores, label = \"eval\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('r2 score')\n",
    "    plt.title('r2 scores over epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
