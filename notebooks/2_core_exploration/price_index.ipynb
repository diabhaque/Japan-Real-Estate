{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from jre_utils.datapath import (\n",
    "    DATA_DIRECTORY_PATH,\n",
    ")\n",
    "from jre_utils.config import asset_types\n",
    "from jp_prefecture.jp_cities import JpCity, jp_cities\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_to_int(area):\n",
    "    if area == \"2,000 m^2 or greater.\":\n",
    "        return 2000\n",
    "    elif area == \"5000 m^2 or greater.\":\n",
    "        return 5000\n",
    "    else:\n",
    "        return int(area)\n",
    "\n",
    "\n",
    "def map_time_units(x):\n",
    "    mapping = {\n",
    "        \"30-60minutes\": 45,\n",
    "        \"1H-1H30\": 75,\n",
    "        \"1H30-2H\": 105,\n",
    "        \"2H-\": 135,\n",
    "    }\n",
    "    \n",
    "    return int(x) if x.isdigit() else mapping[x]\n",
    "\n",
    "def map_layout(x):\n",
    "    if x == \"na\":\n",
    "        return \"na\"\n",
    "\n",
    "    x = x.split(\"+\")[0]\n",
    "\n",
    "    if x[0].isdigit() and int(x[0]) == 1:\n",
    "        if x != \"1K\" and x != \"1LDK\" and x != \"1DK\":\n",
    "            return \"1other\"\n",
    "        return x\n",
    "    \n",
    "    if x[0].isdigit() and int(x[0]) == 2:\n",
    "        if x != \"2LDK\" and x != \"2DK\":\n",
    "            return \"2other\"\n",
    "        return x\n",
    "\n",
    "    if x[0].isdigit() and int(x[0]) > 2:\n",
    "        return f\"{min(int(x[0]), 5)}LDK\"\n",
    "    \n",
    "    return \"other\"\n",
    "\n",
    "def map_land_shape(x):\n",
    "    x = x.lower()\n",
    "    x = x.replace(\"semi-\", \"\")\n",
    "    return x\n",
    "\n",
    "def map_frontage(x):\n",
    "    if x == \"na\":\n",
    "        return 0\n",
    "    if x == \"50.0m or longer.\":\n",
    "        return 55\n",
    "    return int(x.split(\".\")[0])\n",
    "\n",
    "def map_floor_area(x):\n",
    "    if \"less\" in x:\n",
    "        return 10\n",
    "    elif \"greater\" in x:\n",
    "        return 2000\n",
    "    else:\n",
    "        return int(x)\n",
    "    \n",
    "def map_year_of_construction(x):\n",
    "    if x == \"before the war\":\n",
    "        return 1930\n",
    "    \n",
    "    return int(x)\n",
    "\n",
    "def map_building_structure(x):\n",
    "    if len(x.split(\",\")) > 1:\n",
    "        return \"combo\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def map_combined_use(x):\n",
    "    if \"Factory\" in x:\n",
    "        return \"Factory\"\n",
    "    elif \"Warehouse\" in x:\n",
    "        return \"Warehouse\"\n",
    "    elif \"Parking Lot\" in x:\n",
    "        return \"Parking Lot\"\n",
    "    elif \"Office\" in x:\n",
    "        return \"Office\"\n",
    "    elif \"Housing Complex\" in x:\n",
    "        return \"Housing Complex\"\n",
    "    else:\n",
    "        return x.split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'E.g. Maps 1101 to 1100'\n",
      "'E.g. Maps 1100 to Hokkaido Sapporo-shi'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "sub_city_to_city_path = f\"{DATA_DIRECTORY_PATH}/core_scraped/sub_city_to_city.json\"\n",
    "with open(sub_city_to_city_path) as fd:\n",
    "     sub_city_to_city = json.load(fd)\n",
    "     pprint(f\"E.g. Maps 1101 to {sub_city_to_city['1101']}\")\n",
    "\n",
    "area_code_to_area_path = f\"{DATA_DIRECTORY_PATH}/core_scraped/area_code_to_area.json\"\n",
    "with open(area_code_to_area_path) as fd:\n",
    "     area_code_to_area = json.load(fd)\n",
    "     pprint(f\"E.g. Maps 1100 to {area_code_to_area['1100']}\") \n",
    "\n",
    "def get_city_code(area_code):\n",
    "     return sub_city_to_city.get(area_code, area_code)\n",
    "\n",
    "def get_area_from_area_code(area_code):\n",
    "     return area_code_to_area.get(area_code, \"na\" )\n",
    "\n",
    "def get_city_geocode(area_code):\n",
    "    area_code = str(area_code)\n",
    "    try:\n",
    "        return tuple(jp_cities.citycode2geodetic(area_code)) \n",
    "    except:\n",
    "        print(f\"Could not find geocode for {area_code}\")\n",
    "        return np.NaN, np.NaN\n",
    "    \n",
    "def find_town_jp(all_towns_df, city_code, town_name, log=False):\n",
    "    city_df = all_towns_df[all_towns_df[\"cityCode\"] == int(city_code)]\n",
    "    town_df = city_df[city_df[\"townAlphabet\"].str.contains(town_name)]\n",
    "    \n",
    "    if town_df.empty:\n",
    "        if log:\n",
    "            print(f\"JP could not find {town_name} in {city_code}\")\n",
    "        return None, None\n",
    "    \n",
    "    return town_df[\"longitude\"].mean(), town_df[\"latitude\"].mean()\n",
    "\n",
    "def find_town_geopy(geolocator, address, log=False):\n",
    "    location_info = geolocator.geocode(address)\n",
    "\n",
    "    if not location_info:\n",
    "        if log:\n",
    "            print(f\"Geopy could not find {address}\")\n",
    "        return None, None\n",
    "    \n",
    "    return location_info.longitude, location_info.latitude\n",
    "\n",
    "def get_town_coordinates_jp(all_towns_df, city_name, city_code, town_name, log=False):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    \n",
    "    # Try to locade in df - this is fast\n",
    "    jp_lon, jp_lat = find_town_jp(all_towns_df, city_code, town_name, log)\n",
    "\n",
    "    if not jp_lon or not jp_lat:\n",
    "        address = f\"{town_name}, {city_name}, Japan\"\n",
    "        \n",
    "        # Fall back and try to locate with geopy - this is slow\n",
    "        geopy_lon, geopy_lat = find_town_geopy(geolocator, address, log)\n",
    "        \n",
    "        if not geopy_lon or not geopy_lat:\n",
    "            # if nothing works, just return the city coordinates\n",
    "            return get_city_geocode(city_code)\n",
    "        \n",
    "        return geopy_lon, geopy_lat\n",
    "\n",
    "    return jp_lon, jp_lat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/core/13_Tokyo_20053_20233.csv'"
      ]
     },
     "execution_count": 1264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefecture_code = 13\n",
    "trade_prices_data_path = f\"{DATA_DIRECTORY_PATH}/core\"\n",
    "\n",
    "trade_prices_data_paths = [\n",
    "    f\"{trade_prices_data_path}/{filename}\"\n",
    "    for filename in sorted(os.listdir(trade_prices_data_path))\n",
    "]\n",
    "trade_prices_data_paths[prefecture_code - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    trade_prices_data_paths[prefecture_code - 1],\n",
    "    encoding=\"unicode_escape\",\n",
    "    index_col=\"No\",\n",
    ")\n",
    "df[\"area_code\"] = df[\"City,Town,Ward,Village code\"].astype(str)\n",
    "\n",
    "# we may want to skip the following step in the future\n",
    "df[\"area_code\"] = df[\"area_code\"].apply(get_city_code).astype(str)\n",
    "df[\"area\"] = df[\"area_code\"].apply(get_area_from_area_code)\n",
    "\n",
    "df[\"trade_price\"] = df[\"Transaction-price(total)\"]\n",
    "df[\"trade_area\"] = df[\"Area(m^2)\"].apply(area_to_int)\n",
    "df[\"unit_price\"] = df[\"Transaction-price(Unit price m^2)\"]\n",
    "df[\"trade_price_per_area\"] = df[\"trade_price\"] / df[\"trade_area\"]\n",
    "\n",
    "df[\"quarter\"] = df[\"Transaction period\"].apply(lambda x: int(x.split(\" \")[0][0]))\n",
    "df[\"year\"] = df[\"Transaction period\"].apply(lambda x: int(x.split(\" \")[2]))\n",
    "\n",
    "df[\"unit_price\"] = np.where(\n",
    "    df[\"unit_price\"].isna(),\n",
    "    df[\"trade_price_per_area\"],\n",
    "    df[\"unit_price\"],\n",
    ")\n",
    "\n",
    "df = df[\n",
    "    df[\"Type\"].isin(\n",
    "        [\n",
    "            asset_types[\"building\"][\"label\"],\n",
    "            asset_types[\"land\"][\"label\"],\n",
    "            asset_types[\"condo\"][\"label\"],\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "# Renaming\n",
    "\n",
    "df = df.rename(columns = {\n",
    "    \"Type\": \"asset_type\",\n",
    "    \"Region\": \"neighbourhood_classification\",\n",
    "    \"Area\": \"subarea\",\n",
    "    \"Nearest stationFName\": \"nearest_station\",\n",
    "    \"Nearest stationFDistance(minute)\": \"time_to_nearest_station\",\n",
    "    \"Layout\": \"layout\",\n",
    "    \"Land shape\": \"land_shape\",\n",
    "    \"Frontage\": \"frontage\",\n",
    "    \"Total floor area(m^2)\": \"total_floor_area\",\n",
    "    \"Year of construction\": \"year_of_construction\",\n",
    "    \"Building structure\": \"building_structure\",\n",
    "    \"Use\": \"use\",\n",
    "    \"Purpose of Use\": \"purpose\",\n",
    "    \"Frontage roadFDirection\": \"frontage_road_direction\",\n",
    "    \"Frontage roadFClassification\": \"frontage_road_classification\",\n",
    "    \"Frontage roadFBreadth(m)\": \"frontage_road_breadth\",\n",
    "    \"City Planning\": \"zone\",\n",
    "    \"Maximus Building Coverage Ratio(%)\": \"max_building_coverage_ratio\",\n",
    "    \"Maximus Floor-area Ratio(%)\": \"max_floor_area_ratio\",\n",
    "    \"Renovation\": \"renovation_status\",\n",
    "})\n",
    "\n",
    "# Process factors\n",
    "df[\"subarea\"] = df[\"subarea\"].fillna(\"\")\n",
    "df[\"neighbourhood_classification\"] = df[\"neighbourhood_classification\"].fillna(\"na\")\n",
    "df[\"nearest_station\"] = df[\"nearest_station\"].fillna(\"na\")\n",
    "df[\"time_to_nearest_station\"] = df[\"time_to_nearest_station\"].fillna(\"30-60minutes\").apply(map_time_units)\n",
    "df[\"layout\"] = df[\"layout\"].fillna(\"na\").apply(map_layout)\n",
    "df[\"land_shape\"] = df[\"land_shape\"].fillna(\"na\").map(map_land_shape)\n",
    "df[\"frontage\"] = df[\"frontage\"].fillna(\"na\").apply(map_frontage)\n",
    "\n",
    "df[\"total_floor_area\"] = np.where(\n",
    "    df[\"total_floor_area\"].isna(),\n",
    "    df[\"trade_area\"].astype(str),\n",
    "    df[\"total_floor_area\"],\n",
    ")\n",
    "\n",
    "df[\"total_floor_area\"] = df[\"total_floor_area\"].apply(map_floor_area)\n",
    "\n",
    "\n",
    "df[\"year_of_construction\"] = np.where(\n",
    "    df[\"year_of_construction\"].isna(),\n",
    "    (df[\"year\"] - 30).astype(str),\n",
    "    df[\"year_of_construction\"],\n",
    ")\n",
    "\n",
    "df[\"year_of_construction\"] = df[\"year_of_construction\"].apply(map_year_of_construction)\n",
    "df[\"age\"] = df[\"year\"] - df[\"year_of_construction\"]\n",
    "df[\"building_structure\"] = df[\"building_structure\"].fillna(\"na\").map(map_building_structure)\n",
    "df[\"frontage_road_direction\"] = df[\"frontage_road_direction\"].fillna(\"na\")\n",
    "df[\"frontage_road_classification\"] = df[\"frontage_road_classification\"].fillna(\"na\")\n",
    "df[\"frontage_road_breadth\"] = df[\"frontage_road_breadth\"].fillna(\"0.0\").astype(float)\n",
    "df[\"zone\"] = df[\"zone\"].fillna(\"na\")\n",
    "df[\"max_building_coverage_ratio\"] = df[\"max_building_coverage_ratio\"].fillna(0)\n",
    "df[\"max_floor_area_ratio\"] = df[\"max_floor_area_ratio\"].fillna(0)\n",
    "df[\"renovation_status\"] = df[\"renovation_status\"].fillna(\"na\")\n",
    "\n",
    "df[\"combined_use\"] = np.where(\n",
    "    df[\"purpose\"].isna(),\n",
    "    df[\"use\"],\n",
    "    df[\"purpose\"],\n",
    ")\n",
    "df[\"combined_use\"] = df[\"combined_use\"].fillna(\"na\").apply(map_combined_use)\n",
    "\n",
    "df = df.drop(columns = [\n",
    "    \"City,Town,Ward,Village code\",\n",
    "    \"City,Town,Ward,Village\",\n",
    "    \"Transaction-price(total)\",\n",
    "    \"Area(m^2)\",\n",
    "    \"Transaction-price(Unit price m^2)\",\n",
    "    \"trade_price_per_area\",\n",
    "    \"Transaction period\",\n",
    "    \"Prefecture\",\n",
    "    \"Transactional factors\",\n",
    "    \"year_of_construction\", \n",
    "    \"use\",\n",
    "    \"purpose\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced preprocessing\n",
    "\n",
    "# Convert towns into Longitude and Latitude\n",
    "jp_towns = JpCity(enable_town=True)\n",
    "towns_df = jp_towns.towns\n",
    "towns_df[\"townAlphabet\"] = towns_df[\"townAlphabet\"].fillna(\"\")\n",
    "\n",
    "towns_list_df = df[[\"area\", \"area_code\", \"subarea\"]].drop_duplicates()\n",
    "towns_list_df[[\"long\", \"lat\"]] = towns_list_df.apply(\n",
    "    lambda x: pd.Series(get_town_coordinates_jp(towns_df, x[\"area\"], x[\"area_code\"], x[\"subarea\"])),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df = df.merge(towns_list_df, on=[\"area\", \"area_code\", \"subarea\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each asset type, for each area_code\n",
    "# 1. Run clustering algorithm and get cluster code for each transaction\n",
    "# 2. Run regression to identify weights for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Land columns\n",
    "\n",
    "numerical_columns = [\n",
    "    \"long\",\n",
    "    \"lat\",\n",
    "    \"time_to_nearest_station\",\n",
    "    \"total_floor_area\",\n",
    "    \"trade_area\",\n",
    "    \"age\",\n",
    "    \"frontage\",\n",
    "    \"frontage_road_breadth\",\n",
    "    \"max_building_coverage_ratio\",\n",
    "    \"max_floor_area_ratio\",\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    \"neighbourhood_classification\",\n",
    "    \"quarter\",\n",
    "    \"zone\",\n",
    "    \"renovation_status\",\n",
    "    \"combined_use\",\n",
    "    \"layout\",\n",
    "    \"building_structure\",\n",
    "    \"land_shape\",\n",
    "    # \"frontage_road_direction\",\n",
    "    # \"frontage_road_classification\",\n",
    "]\n",
    "\n",
    "id_columns = [\"year\"]\n",
    "metric_columns = [\"unit_price_log\"]\n",
    "\n",
    "columns = numerical_columns + categorical_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_df = df[df[\"asset_type\"] == asset_types[\"land\"][\"label\"]]\n",
    "building_df = df[df[\"asset_type\"] == asset_types[\"building\"][\"label\"]]\n",
    "condo_df = df[df[\"asset_type\"] == asset_types[\"condo\"][\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area_df = land_df[land_df[\"area_code\"] == \"13101\"].reset_index(drop=True)\n",
    "area_df = building_df[building_df[\"area_code\"] == \"13101\"].reset_index(drop=True)\n",
    "area_df[\"year\"] = area_df[\"year\"].astype(str)\n",
    "area_df[\"quarter\"] = area_df[\"quarter\"].astype(str)\n",
    "area_df[f\"unit_price_log\"] = np.log(area_df[\"unit_price\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1429,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1429], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Scale numerical variables\u001b[39;00m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 6\u001b[0m area_df[numerical_columns] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43marea_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumerical_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Categorical variables\u001b[39;00m\n\u001b[1;32m      9\u001b[0m area_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(area_df[columns \u001b[38;5;241m+\u001b[39m id_columns \u001b[38;5;241m+\u001b[39m metric_columns], columns\u001b[38;5;241m=\u001b[39mcategorical_columns)\n",
      "File \u001b[0;32m~/Desktop/japan_re/venv38/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/japan_re/venv38/lib/python3.8/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Desktop/japan_re/venv38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/japan_re/venv38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/japan_re/venv38/lib/python3.8/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/Desktop/japan_re/venv38/lib/python3.8/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/Desktop/japan_re/venv38/lib/python3.8/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for column in numerical_columns:\n",
    "    area_df[column] = np.log(area_df[column] + 1)\n",
    "\n",
    "# Scale numerical variables\n",
    "scaler = StandardScaler()\n",
    "area_df[numerical_columns] = scaler.fit_transform(area_df[numerical_columns])\n",
    "\n",
    "# Categorical variables\n",
    "area_df = pd.get_dummies(area_df[columns + id_columns + metric_columns], columns=categorical_columns)\n",
    "drop_columns = [col for col in area_df.columns if \"na\" in col]\n",
    "area_df = area_df.drop(columns=drop_columns)\n",
    "area_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1424,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ord = area_df.drop(columns=id_columns + metric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components retained by PCA: 9\n",
      "(315, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.90)  # Retain 95% of the variance\n",
    "X_pca = pca.fit_transform(X_ord)\n",
    "\n",
    "# Print the number of components retained by PCA\n",
    "print(\"Number of components retained by PCA:\", pca.n_components_)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_price_log</th>\n",
       "      <th>year_2005</th>\n",
       "      <th>year_2006</th>\n",
       "      <th>year_2007</th>\n",
       "      <th>year_2008</th>\n",
       "      <th>year_2009</th>\n",
       "      <th>year_2010</th>\n",
       "      <th>year_2011</th>\n",
       "      <th>year_2012</th>\n",
       "      <th>year_2013</th>\n",
       "      <th>year_2014</th>\n",
       "      <th>year_2015</th>\n",
       "      <th>year_2016</th>\n",
       "      <th>year_2017</th>\n",
       "      <th>year_2018</th>\n",
       "      <th>year_2019</th>\n",
       "      <th>year_2020</th>\n",
       "      <th>year_2021</th>\n",
       "      <th>year_2022</th>\n",
       "      <th>year_2023</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.687313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296387</td>\n",
       "      <td>0.205170</td>\n",
       "      <td>-0.388837</td>\n",
       "      <td>1.506292</td>\n",
       "      <td>0.163370</td>\n",
       "      <td>3.002410</td>\n",
       "      <td>-0.147497</td>\n",
       "      <td>-0.227663</td>\n",
       "      <td>-0.962639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.404746</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.320348</td>\n",
       "      <td>-0.039934</td>\n",
       "      <td>-0.214520</td>\n",
       "      <td>0.802440</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>1.098975</td>\n",
       "      <td>0.126730</td>\n",
       "      <td>-0.905790</td>\n",
       "      <td>-0.361875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.607270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867513</td>\n",
       "      <td>-0.030192</td>\n",
       "      <td>-0.374634</td>\n",
       "      <td>0.625158</td>\n",
       "      <td>-0.087014</td>\n",
       "      <td>-0.351827</td>\n",
       "      <td>0.647694</td>\n",
       "      <td>-0.724316</td>\n",
       "      <td>0.178729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.845130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.927809</td>\n",
       "      <td>1.469196</td>\n",
       "      <td>0.955657</td>\n",
       "      <td>0.457797</td>\n",
       "      <td>0.461749</td>\n",
       "      <td>0.358570</td>\n",
       "      <td>1.135893</td>\n",
       "      <td>0.324113</td>\n",
       "      <td>0.231455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.201805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.336708</td>\n",
       "      <td>-0.821013</td>\n",
       "      <td>-0.607515</td>\n",
       "      <td>1.519816</td>\n",
       "      <td>1.444096</td>\n",
       "      <td>-0.445919</td>\n",
       "      <td>0.157887</td>\n",
       "      <td>0.629655</td>\n",
       "      <td>-0.305895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>14.690980</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.153528</td>\n",
       "      <td>1.555820</td>\n",
       "      <td>1.196973</td>\n",
       "      <td>-0.813743</td>\n",
       "      <td>-0.556498</td>\n",
       "      <td>-0.372107</td>\n",
       "      <td>1.277803</td>\n",
       "      <td>0.113822</td>\n",
       "      <td>0.533719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>13.795309</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.633113</td>\n",
       "      <td>1.710122</td>\n",
       "      <td>0.489382</td>\n",
       "      <td>0.223671</td>\n",
       "      <td>-0.804513</td>\n",
       "      <td>1.592425</td>\n",
       "      <td>0.091620</td>\n",
       "      <td>-0.714876</td>\n",
       "      <td>-0.431253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>14.508658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.135261</td>\n",
       "      <td>0.140575</td>\n",
       "      <td>0.147155</td>\n",
       "      <td>-1.274966</td>\n",
       "      <td>-0.068390</td>\n",
       "      <td>-0.091653</td>\n",
       "      <td>-0.582197</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>-0.838588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>11.918397</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.190307</td>\n",
       "      <td>1.926048</td>\n",
       "      <td>0.827396</td>\n",
       "      <td>-1.938729</td>\n",
       "      <td>-3.203943</td>\n",
       "      <td>-0.703787</td>\n",
       "      <td>-0.562088</td>\n",
       "      <td>0.471184</td>\n",
       "      <td>0.850269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>14.557448</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.922498</td>\n",
       "      <td>1.612016</td>\n",
       "      <td>0.949834</td>\n",
       "      <td>-0.274036</td>\n",
       "      <td>-1.016647</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>0.204493</td>\n",
       "      <td>0.689261</td>\n",
       "      <td>1.165922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unit_price_log  year_2005  year_2006  year_2007  year_2008  year_2009  \\\n",
       "0         15.687313          0          0          0          0          0   \n",
       "1         15.404746          0          0          0          0          0   \n",
       "2         15.607270          0          0          0          0          0   \n",
       "3         14.845130          0          0          0          0          0   \n",
       "4         15.201805          0          0          0          0          0   \n",
       "..              ...        ...        ...        ...        ...        ...   \n",
       "310       14.690980          0          1          0          0          0   \n",
       "311       13.795309          0          1          0          0          0   \n",
       "312       14.508658          0          0          1          0          0   \n",
       "313       11.918397          1          0          0          0          0   \n",
       "314       14.557448          1          0          0          0          0   \n",
       "\n",
       "     year_2010  year_2011  year_2012  year_2013  year_2014  year_2015  \\\n",
       "0            0          0          0          0          0          0   \n",
       "1            0          0          0          0          0          0   \n",
       "2            0          0          0          0          0          0   \n",
       "3            0          0          0          0          0          0   \n",
       "4            0          0          0          0          0          0   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "310          0          0          0          0          0          0   \n",
       "311          0          0          0          0          0          0   \n",
       "312          0          0          0          0          0          0   \n",
       "313          0          0          0          0          0          0   \n",
       "314          0          0          0          0          0          0   \n",
       "\n",
       "     year_2016  year_2017  year_2018  year_2019  year_2020  year_2021  \\\n",
       "0            0          0          0          0          0          0   \n",
       "1            0          0          0          0          0          0   \n",
       "2            0          0          0          0          0          0   \n",
       "3            0          0          0          1          0          0   \n",
       "4            0          0          0          0          0          1   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "310          0          0          0          0          0          0   \n",
       "311          0          0          0          0          0          0   \n",
       "312          0          0          0          0          0          0   \n",
       "313          0          0          0          0          0          0   \n",
       "314          0          0          0          0          0          0   \n",
       "\n",
       "     year_2022  year_2023       PC1       PC2       PC3       PC4       PC5  \\\n",
       "0            0          1  0.296387  0.205170 -0.388837  1.506292  0.163370   \n",
       "1            0          1 -0.320348 -0.039934 -0.214520  0.802440  0.007693   \n",
       "2            0          1  0.867513 -0.030192 -0.374634  0.625158 -0.087014   \n",
       "3            0          0 -1.927809  1.469196  0.955657  0.457797  0.461749   \n",
       "4            0          0 -0.336708 -0.821013 -0.607515  1.519816  1.444096   \n",
       "..         ...        ...       ...       ...       ...       ...       ...   \n",
       "310          0          0 -2.153528  1.555820  1.196973 -0.813743 -0.556498   \n",
       "311          0          0  0.633113  1.710122  0.489382  0.223671 -0.804513   \n",
       "312          0          0 -1.135261  0.140575  0.147155 -1.274966 -0.068390   \n",
       "313          0          0  1.190307  1.926048  0.827396 -1.938729 -3.203943   \n",
       "314          0          0 -0.922498  1.612016  0.949834 -0.274036 -1.016647   \n",
       "\n",
       "          PC6       PC7       PC8       PC9  \n",
       "0    3.002410 -0.147497 -0.227663 -0.962639  \n",
       "1    1.098975  0.126730 -0.905790 -0.361875  \n",
       "2   -0.351827  0.647694 -0.724316  0.178729  \n",
       "3    0.358570  1.135893  0.324113  0.231455  \n",
       "4   -0.445919  0.157887  0.629655 -0.305895  \n",
       "..        ...       ...       ...       ...  \n",
       "310 -0.372107  1.277803  0.113822  0.533719  \n",
       "311  1.592425  0.091620 -0.714876 -0.431253  \n",
       "312 -0.091653 -0.582197 -0.281464 -0.838588  \n",
       "313 -0.703787 -0.562088  0.471184  0.850269  \n",
       "314 -0.009618  0.204493  0.689261  1.165922  \n",
       "\n",
       "[315 rows x 29 columns]"
      ]
     },
     "execution_count": 1426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearly_df = area_df[id_columns + metric_columns]\n",
    "yearly_df = pd.get_dummies(yearly_df, columns=id_columns)\n",
    "yearly_df = pd.concat(\n",
    "    [\n",
    "        yearly_df,\n",
    "        pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(X_pca.shape[1])]),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "yearly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         unit_price_log   R-squared:                       0.594\n",
      "Model:                            OLS   Adj. R-squared:                  0.556\n",
      "Method:                 Least Squares   F-statistic:                     15.54\n",
      "Date:                Thu, 28 Mar 2024   Prob (F-statistic):           1.05e-41\n",
      "Time:                        16:12:44   Log-Likelihood:                -207.12\n",
      "No. Observations:                 315   AIC:                             470.2\n",
      "Df Residuals:                     287   BIC:                             575.3\n",
      "Df Model:                          27                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         13.7502      0.030    459.123      0.000      13.691      13.809\n",
      "year_2005      0.1533      0.139      1.107      0.269      -0.119       0.426\n",
      "year_2006      0.4119      0.093      4.426      0.000       0.229       0.595\n",
      "year_2007      0.7297      0.101      7.209      0.000       0.530       0.929\n",
      "year_2008      0.9504      0.145      6.567      0.000       0.666       1.235\n",
      "year_2009      0.3894      0.134      2.897      0.004       0.125       0.654\n",
      "year_2010      0.1490      0.099      1.511      0.132      -0.045       0.343\n",
      "year_2011      0.1239      0.094      1.314      0.190      -0.062       0.309\n",
      "year_2012      0.1054      0.121      0.873      0.383      -0.132       0.343\n",
      "year_2013      0.4713      0.100      4.699      0.000       0.274       0.669\n",
      "year_2014      0.4495      0.115      3.922      0.000       0.224       0.675\n",
      "year_2015      0.5912      0.116      5.103      0.000       0.363       0.819\n",
      "year_2016      0.7496      0.124      6.068      0.000       0.506       0.993\n",
      "year_2017      1.0395      0.134      7.733      0.000       0.775       1.304\n",
      "year_2018      0.9485      0.235      4.037      0.000       0.486       1.411\n",
      "year_2019      1.0604      0.133      7.953      0.000       0.798       1.323\n",
      "year_2020      1.2188      0.107     11.407      0.000       1.009       1.429\n",
      "year_2021      1.2500      0.147      8.515      0.000       0.961       1.539\n",
      "year_2022      1.0899      0.158      6.888      0.000       0.778       1.401\n",
      "year_2023      1.8686      0.198      9.440      0.000       1.479       2.258\n",
      "PC1           -0.1954      0.018    -11.047      0.000      -0.230      -0.161\n",
      "PC2           -0.0612      0.020     -3.111      0.002      -0.100      -0.022\n",
      "PC3            0.0150      0.021      0.721      0.471      -0.026       0.056\n",
      "PC4            0.1352      0.029      4.708      0.000       0.079       0.192\n",
      "PC5            0.2898      0.029      9.894      0.000       0.232       0.347\n",
      "PC6           -0.0809      0.040     -2.045      0.042      -0.159      -0.003\n",
      "PC7            0.0900      0.052      1.745      0.082      -0.011       0.191\n",
      "PC8            0.0444      0.057      0.777      0.438      -0.068       0.157\n",
      "PC9            0.0879      0.058      1.505      0.134      -0.027       0.203\n",
      "==============================================================================\n",
      "Omnibus:                       28.392   Durbin-Watson:                   1.767\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               43.527\n",
      "Skew:                          -0.589   Prob(JB):                     3.53e-10\n",
      "Kurtosis:                       4.389   Cond. No.                     9.56e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 9.16e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = yearly_df.drop(columns=metric_columns)\n",
    "y = yearly_df[metric_columns[0]]\n",
    "\n",
    "# Add a constant to the model (the intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "year_columns = [col for col in X.columns if \"year\" in col]\n",
    "# Get the weights (coefficients) for each year and PCA component\n",
    "year_weights = model.coef_[:len(year_columns)]\n",
    "pca_weights = model.coef_[len(year_columns):]\n",
    "\n",
    "# Create a DataFrame to store the year weights (price index)\n",
    "# year_weights_df = pd.DataFrame({'Year': year_encoder.categories_[0][1:], 'Weight': year_weights})\n",
    "# year_weights_df.set_index('Year', inplace=True)\n",
    "\n",
    "# # Print the year weights (price index)\n",
    "# print(\"Year Weights (Price Index):\")\n",
    "# print(year_weights_df)\n",
    "\n",
    "# # Print the PCA component weights\n",
    "# print(\"\\nPCA Component Weights:\")\n",
    "# for i, weight in enumerate(pca_weights, start=1):\n",
    "#     print(f\"PC{i}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87822706e+08,  5.24127037e+11,  5.24127037e+11,  5.24127037e+11,\n",
       "        5.24127037e+11,  5.24127037e+11,  5.24127037e+11,  5.24127037e+11,\n",
       "        5.24127037e+11,  5.24127037e+11,  5.24127037e+11,  5.24127037e+11,\n",
       "        5.24127037e+11,  5.24127037e+11,  5.24127037e+11,  5.24127037e+11,\n",
       "        5.24127037e+11,  5.24127037e+11,  5.24127037e+11])"
      ]
     },
     "execution_count": 1369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jre_utils.visualize import plot_time_series\n",
    "\n",
    "\n",
    "# plot_time_series(combined_cluster_df, column=\"annualized_return\", group_by_columns=[\"cluster_code\", \"year\"], granularity_columns=[\"cluster_code\"], title=\"Returns by cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I have a dense vector - I can append my year dummies and compute the returns for each year + pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
