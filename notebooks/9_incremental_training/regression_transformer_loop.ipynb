{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "from jre_utils.config import asset_types\n",
    "from jre_utils.datapath import model_ready_data_paths, model_output_data_paths\n",
    "from jre_utils.process import get_most_active_municipalities\n",
    "from jre_utils.data import JapanRETimeSeriesDataset, PadAndMask, ToNumpy, ToTensor\n",
    "from jre_utils.models import TimeSeriesTransformerModel\n",
    "from jre_utils.engine import (\n",
    "    evaluate_weighted,\n",
    "    train_weighted,\n",
    ")\n",
    "from jre_utils.backtest import predict_returns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# only train on top 500 municipalities\n",
    "dataset_asset_type = \"combined\"\n",
    "dataset_key = \"transactions\"\n",
    "years_ahead = 2\n",
    "\n",
    "metrics = {\n",
    "    \"median\": \"unit_price_median\",\n",
    "    \"gmean\": \"unit_price_gmean\",\n",
    "    \"robust\": \"robust_price_index\",\n",
    "    \"ols\": \"ols_price_index\",\n",
    "}\n",
    "\n",
    "granularity_columns = [\"area\", \"area_code\"]\n",
    "group_by_columns = granularity_columns + [\"year\"]\n",
    "\n",
    "metric_key = \"robust\"\n",
    "metric = metrics[metric_key]\n",
    "metric_sharpe = metric + \"_sharpe\"\n",
    "normalized_metric_sharpe = metric_sharpe + \"_normalized_yearly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_invalid_rows(df, column):\n",
    "    return df[~df[column].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = [\"area_code\", \"area\", \"year\"]\n",
    "\n",
    "original_factor_columns = [\n",
    "    \"taxable_income_growth\",\n",
    "    \"taxable_income_per_taxpayer_growth\",\n",
    "    \"net_migration_ratio\",\n",
    "    \"new_dwellings_ratio\",\n",
    "    \"taxpayer_count_growth\",\n",
    "]\n",
    "\n",
    "factor_log_normalize_columns = [\n",
    "    \"population\",\n",
    "    \"taxable_income_growth\",\n",
    "    \"taxable_income_per_taxpayer_growth\",\n",
    "    \"net_migration_ratio\",\n",
    "    \"new_dwellings_ratio\",\n",
    "    \"taxpayer_count_growth\",\n",
    "]\n",
    "\n",
    "factor_normalize_columns = []\n",
    "\n",
    "factor_maintain_columns = [\n",
    "    \"migrations_is_available\",\n",
    "    \"taxable_income_is_available\",\n",
    "    \"dwellings_is_available\",\n",
    "    \"total_tax_is_available\",\n",
    "]\n",
    "\n",
    "factor_columns = (\n",
    "    [f\"{column}_log_normalized_yearly\" for column in factor_log_normalize_columns]\n",
    "    + [f\"{column}_normalized_yearly\" for column in factor_normalize_columns]\n",
    "    + factor_maintain_columns\n",
    "    # + original_factor_columns\n",
    ")\n",
    "\n",
    "final_factor_columns = (\n",
    "    factor_normalize_columns + factor_log_normalize_columns + factor_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asset_types_to_train = list(asset_types.keys())\n",
    "asset_types_to_train = [\"building\"]\n",
    "asset_types_as_factors = list(set(asset_types_to_train + [\"building\"]))\n",
    "\n",
    "core_log_normalize_columns = []\n",
    "core_normalize_columns = [metric_sharpe]\n",
    "core_maintain_columns = [\"yearly_price_growth\", \"metric_sharpe_is_available\"]\n",
    "\n",
    "core_columns = (\n",
    "    [f\"{column}_log_normalized_yearly\" for column in core_log_normalize_columns]\n",
    "    + [f\"{column}_normalized_yearly\" for column in core_normalize_columns]\n",
    "    + core_maintain_columns\n",
    ")\n",
    "\n",
    "combined_core_columns = [\n",
    "    f\"{asset_type}_{column}\"\n",
    "    for column in core_columns\n",
    "    for asset_type in asset_types_as_factors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = factor_columns + combined_core_columns + [\"land\", \"condo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2006\n",
    "end_year = 2022\n",
    "train_start_year = 2008\n",
    "train_end_year = 2020\n",
    "\n",
    "dataset_name = f\"sequence_{dataset_key}_{dataset_asset_type}_{metric_key}_{years_ahead}\"\n",
    "model_ready_data_path = model_ready_data_paths[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(feature_columns)\n",
    "d_model = 128\n",
    "d_hid = 128\n",
    "nlayers = 4\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "enc_dropout = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4 # 3e-4\n",
    "weight_decay = 1 # 1\n",
    "num_epochs_per_year = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      " Horizon: 4\n",
      "-----------------\n",
      "-----------------\n",
      " Year: 2010\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 14.6122, Eval Loss: 7.6869\n",
      " Train R^2: -0.7119, Eval R^2: -0.1201\n",
      " Epoch: 1\n",
      " Train Loss: 10.5127, Eval Loss: 6.9466\n",
      " Train R^2: -0.2539, Eval R^2: -0.0206\n",
      " Epoch: 2\n",
      " Train Loss: 8.7284, Eval Loss: 7.1145\n",
      " Train R^2: -0.0826, Eval R^2: -0.0614\n",
      " Epoch: 3\n",
      " Train Loss: 7.7819, Eval Loss: 7.7450\n",
      " Train R^2: 0.0019, Eval R^2: -0.1714\n",
      " Epoch: 4\n",
      " Train Loss: 7.9820, Eval Loss: 8.3705\n",
      " Train R^2: -0.0513, Eval R^2: -0.2763\n",
      " Epoch: 5\n",
      " Train Loss: 9.0106, Eval Loss: 8.7215\n",
      " Train R^2: -0.1740, Eval R^2: -0.3342\n",
      " Epoch: 6\n",
      " Train Loss: 8.9134, Eval Loss: 8.7788\n",
      " Train R^2: -0.1807, Eval R^2: -0.3434\n",
      " Epoch: 7\n",
      " Train Loss: 8.8388, Eval Loss: 8.6395\n",
      " Train R^2: -0.1729, Eval R^2: -0.3201\n",
      " Epoch: 8\n",
      " Train Loss: 8.7180, Eval Loss: 8.3955\n",
      " Train R^2: -0.1525, Eval R^2: -0.2795\n",
      " Epoch: 9\n",
      " Train Loss: 8.6045, Eval Loss: 8.1139\n",
      " Train R^2: -0.1383, Eval R^2: -0.2324\n",
      " Epoch: 10\n",
      " Train Loss: 8.5602, Eval Loss: 7.8444\n",
      " Train R^2: -0.1294, Eval R^2: -0.1872\n",
      " Epoch: 11\n",
      " Train Loss: 7.9145, Eval Loss: 7.6137\n",
      " Train R^2: -0.0327, Eval R^2: -0.1483\n",
      " Epoch: 12\n",
      " Train Loss: 8.0751, Eval Loss: 7.4301\n",
      " Train R^2: -0.0534, Eval R^2: -0.1170\n",
      " Epoch: 13\n",
      " Train Loss: 7.7639, Eval Loss: 7.2913\n",
      " Train R^2: -0.0064, Eval R^2: -0.0931\n",
      " Epoch: 14\n",
      " Train Loss: 7.6680, Eval Loss: 7.1907\n",
      " Train R^2: -0.0005, Eval R^2: -0.0756\n",
      " Epoch: 15\n",
      " Train Loss: 7.9248, Eval Loss: 7.1220\n",
      " Train R^2: -0.0128, Eval R^2: -0.0636\n",
      " Epoch: 16\n",
      " Train Loss: 7.4785, Eval Loss: 7.0762\n",
      " Train R^2: 0.0399, Eval R^2: -0.0554\n",
      " Epoch: 17\n",
      " Train Loss: 7.8490, Eval Loss: 7.0474\n",
      " Train R^2: 0.0002, Eval R^2: -0.0503\n",
      " Epoch: 18\n",
      " Train Loss: 7.7076, Eval Loss: 7.0310\n",
      " Train R^2: 0.0056, Eval R^2: -0.0473\n",
      " Epoch: 19\n",
      " Train Loss: 7.5046, Eval Loss: 7.0239\n",
      " Train R^2: 0.0368, Eval R^2: -0.0460\n",
      "-----------------\n",
      " Year: 2011\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 10.2418, Eval Loss: 11.8502\n",
      " Train R^2: -0.7946, Eval R^2: -0.4733\n",
      " Epoch: 1\n",
      " Train Loss: 7.8564, Eval Loss: 10.1085\n",
      " Train R^2: -0.3717, Eval R^2: -0.2474\n",
      " Epoch: 2\n",
      " Train Loss: 6.5982, Eval Loss: 9.1680\n",
      " Train R^2: -0.1282, Eval R^2: -0.1258\n",
      " Epoch: 3\n",
      " Train Loss: 6.1930, Eval Loss: 8.7997\n",
      " Train R^2: -0.0249, Eval R^2: -0.0786\n",
      " Epoch: 4\n",
      " Train Loss: 6.4388, Eval Loss: 8.7172\n",
      " Train R^2: -0.0289, Eval R^2: -0.0683\n",
      " Epoch: 5\n",
      " Train Loss: 6.7020, Eval Loss: 8.7104\n",
      " Train R^2: -0.0504, Eval R^2: -0.0676\n",
      " Epoch: 6\n",
      " Train Loss: 6.9515, Eval Loss: 8.7060\n",
      " Train R^2: -0.0826, Eval R^2: -0.0667\n",
      " Epoch: 7\n",
      " Train Loss: 6.8753, Eval Loss: 8.7046\n",
      " Train R^2: -0.0753, Eval R^2: -0.0660\n",
      " Epoch: 8\n",
      " Train Loss: 6.8006, Eval Loss: 8.7222\n",
      " Train R^2: -0.0616, Eval R^2: -0.0674\n",
      " Epoch: 9\n",
      " Train Loss: 6.5701, Eval Loss: 8.7693\n",
      " Train R^2: -0.0382, Eval R^2: -0.0726\n",
      " Epoch: 10\n",
      " Train Loss: 6.4165, Eval Loss: 8.8451\n",
      " Train R^2: -0.0218, Eval R^2: -0.0814\n",
      " Epoch: 11\n",
      " Train Loss: 6.1854, Eval Loss: 8.9411\n",
      " Train R^2: 0.0011, Eval R^2: -0.0928\n",
      " Epoch: 12\n",
      " Train Loss: 6.1533, Eval Loss: 9.0460\n",
      " Train R^2: 0.0016, Eval R^2: -0.1055\n",
      " Epoch: 13\n",
      " Train Loss: 6.0931, Eval Loss: 9.1493\n",
      " Train R^2: 0.0058, Eval R^2: -0.1180\n",
      " Epoch: 14\n",
      " Train Loss: 6.0792, Eval Loss: 9.2417\n",
      " Train R^2: 0.0043, Eval R^2: -0.1293\n",
      " Epoch: 15\n",
      " Train Loss: 6.0385, Eval Loss: 9.3182\n",
      " Train R^2: 0.0099, Eval R^2: -0.1386\n",
      " Epoch: 16\n",
      " Train Loss: 6.1203, Eval Loss: 9.3765\n",
      " Train R^2: -0.0126, Eval R^2: -0.1457\n",
      " Epoch: 17\n",
      " Train Loss: 6.0166, Eval Loss: 9.4158\n",
      " Train R^2: -0.0006, Eval R^2: -0.1505\n",
      " Epoch: 18\n",
      " Train Loss: 6.1241, Eval Loss: 9.4389\n",
      " Train R^2: -0.0178, Eval R^2: -0.1533\n",
      " Epoch: 19\n",
      " Train Loss: 6.1285, Eval Loss: 9.4488\n",
      " Train R^2: -0.0207, Eval R^2: -0.1545\n",
      "-----------------\n",
      " Year: 2012\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 7.3556, Eval Loss: 11.6732\n",
      " Train R^2: -0.0438, Eval R^2: -0.2705\n",
      " Epoch: 1\n",
      " Train Loss: 7.1507, Eval Loss: 10.3329\n",
      " Train R^2: -0.0413, Eval R^2: -0.1233\n",
      " Epoch: 2\n",
      " Train Loss: 6.9389, Eval Loss: 9.8954\n",
      " Train R^2: 0.0117, Eval R^2: -0.0797\n",
      " Epoch: 3\n",
      " Train Loss: 6.9263, Eval Loss: 10.5102\n",
      " Train R^2: 0.0136, Eval R^2: -0.1386\n",
      " Epoch: 4\n",
      " Train Loss: 6.8617, Eval Loss: 11.2848\n",
      " Train R^2: 0.0164, Eval R^2: -0.2164\n",
      " Epoch: 5\n",
      " Train Loss: 6.8532, Eval Loss: 11.1960\n",
      " Train R^2: 0.0132, Eval R^2: -0.2055\n",
      " Epoch: 6\n",
      " Train Loss: 6.8117, Eval Loss: 10.7945\n",
      " Train R^2: 0.0223, Eval R^2: -0.1643\n",
      " Epoch: 7\n",
      " Train Loss: 6.8182, Eval Loss: 10.7048\n",
      " Train R^2: 0.0266, Eval R^2: -0.1553\n",
      " Epoch: 8\n",
      " Train Loss: 6.8066, Eval Loss: 10.9489\n",
      " Train R^2: 0.0285, Eval R^2: -0.1780\n",
      " Epoch: 9\n",
      " Train Loss: 6.7397, Eval Loss: 11.2082\n",
      " Train R^2: 0.0344, Eval R^2: -0.2024\n",
      " Epoch: 10\n",
      " Train Loss: 6.7807, Eval Loss: 11.2316\n",
      " Train R^2: 0.0290, Eval R^2: -0.2039\n",
      " Epoch: 11\n",
      " Train Loss: 6.7406, Eval Loss: 11.1126\n",
      " Train R^2: 0.0333, Eval R^2: -0.1919\n",
      " Epoch: 12\n",
      " Train Loss: 6.7428, Eval Loss: 11.0154\n",
      " Train R^2: 0.0353, Eval R^2: -0.1825\n",
      " Epoch: 13\n",
      " Train Loss: 6.7200, Eval Loss: 11.0179\n",
      " Train R^2: 0.0394, Eval R^2: -0.1824\n",
      " Epoch: 14\n",
      " Train Loss: 6.6933, Eval Loss: 11.0903\n",
      " Train R^2: 0.0418, Eval R^2: -0.1888\n",
      " Epoch: 15\n",
      " Train Loss: 6.7618, Eval Loss: 11.1554\n",
      " Train R^2: 0.0357, Eval R^2: -0.1946\n",
      " Epoch: 16\n",
      " Train Loss: 6.6929, Eval Loss: 11.1708\n",
      " Train R^2: 0.0413, Eval R^2: -0.1958\n",
      " Epoch: 17\n",
      " Train Loss: 6.7399, Eval Loss: 11.1540\n",
      " Train R^2: 0.0343, Eval R^2: -0.1940\n",
      " Epoch: 18\n",
      " Train Loss: 6.7386, Eval Loss: 11.1305\n",
      " Train R^2: 0.0342, Eval R^2: -0.1918\n",
      " Epoch: 19\n",
      " Train Loss: 6.6831, Eval Loss: 11.1190\n",
      " Train R^2: 0.0428, Eval R^2: -0.1907\n",
      "-----------------\n",
      " Year: 2013\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 7.5786, Eval Loss: 9.6741\n",
      " Train R^2: -0.0597, Eval R^2: -0.1498\n",
      " Epoch: 1\n",
      " Train Loss: 7.1117, Eval Loss: 8.5789\n",
      " Train R^2: -0.0044, Eval R^2: -0.0478\n",
      " Epoch: 2\n",
      " Train Loss: 7.1043, Eval Loss: 9.2425\n",
      " Train R^2: 0.0055, Eval R^2: -0.1064\n",
      " Epoch: 3\n",
      " Train Loss: 7.0289, Eval Loss: 9.4744\n",
      " Train R^2: 0.0105, Eval R^2: -0.1311\n",
      " Epoch: 4\n",
      " Train Loss: 6.9702, Eval Loss: 9.0958\n",
      " Train R^2: 0.0188, Eval R^2: -0.0998\n",
      " Epoch: 5\n",
      " Train Loss: 7.0377, Eval Loss: 9.2403\n",
      " Train R^2: 0.0132, Eval R^2: -0.1123\n",
      " Epoch: 6\n",
      " Train Loss: 6.9467, Eval Loss: 9.3753\n",
      " Train R^2: 0.0226, Eval R^2: -0.1237\n",
      " Epoch: 7\n",
      " Train Loss: 6.9639, Eval Loss: 9.2172\n",
      " Train R^2: 0.0191, Eval R^2: -0.1114\n",
      " Epoch: 8\n",
      " Train Loss: 6.9529, Eval Loss: 9.1760\n",
      " Train R^2: 0.0219, Eval R^2: -0.1089\n",
      " Epoch: 9\n",
      " Train Loss: 6.9488, Eval Loss: 9.2371\n",
      " Train R^2: 0.0226, Eval R^2: -0.1129\n",
      " Epoch: 10\n",
      " Train Loss: 6.9643, Eval Loss: 9.2240\n",
      " Train R^2: 0.0194, Eval R^2: -0.1120\n",
      " Epoch: 11\n",
      " Train Loss: 6.9119, Eval Loss: 9.1812\n",
      " Train R^2: 0.0276, Eval R^2: -0.1097\n",
      " Epoch: 12\n",
      " Train Loss: 6.8705, Eval Loss: 9.1771\n",
      " Train R^2: 0.0334, Eval R^2: -0.1097\n",
      " Epoch: 13\n",
      " Train Loss: 6.9179, Eval Loss: 9.2023\n",
      " Train R^2: 0.0262, Eval R^2: -0.1115\n",
      " Epoch: 14\n",
      " Train Loss: 6.8698, Eval Loss: 9.2031\n",
      " Train R^2: 0.0356, Eval R^2: -0.1119\n",
      " Epoch: 15\n",
      " Train Loss: 6.8860, Eval Loss: 9.1914\n",
      " Train R^2: 0.0304, Eval R^2: -0.1115\n",
      " Epoch: 16\n",
      " Train Loss: 6.9598, Eval Loss: 9.1845\n",
      " Train R^2: 0.0220, Eval R^2: -0.1114\n",
      " Epoch: 17\n",
      " Train Loss: 6.9033, Eval Loss: 9.1888\n",
      " Train R^2: 0.0311, Eval R^2: -0.1118\n",
      " Epoch: 18\n",
      " Train Loss: 6.8791, Eval Loss: 9.1966\n",
      " Train R^2: 0.0319, Eval R^2: -0.1123\n",
      " Epoch: 19\n",
      " Train Loss: 6.8735, Eval Loss: 9.1991\n",
      " Train R^2: 0.0333, Eval R^2: -0.1124\n",
      "-----------------\n",
      " Year: 2014\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 7.1669, Eval Loss: 13.0058\n",
      " Train R^2: -0.0263, Eval R^2: -0.2251\n",
      " Epoch: 1\n",
      " Train Loss: 7.1127, Eval Loss: 11.6622\n",
      " Train R^2: -0.0201, Eval R^2: -0.1081\n",
      " Epoch: 2\n",
      " Train Loss: 7.0080, Eval Loss: 12.7281\n",
      " Train R^2: 0.0035, Eval R^2: -0.1915\n",
      " Epoch: 3\n",
      " Train Loss: 6.9455, Eval Loss: 11.8574\n",
      " Train R^2: 0.0071, Eval R^2: -0.1155\n",
      " Epoch: 4\n",
      " Train Loss: 6.8736, Eval Loss: 12.0332\n",
      " Train R^2: 0.0205, Eval R^2: -0.1282\n",
      " Epoch: 5\n",
      " Train Loss: 6.8971, Eval Loss: 12.0235\n",
      " Train R^2: 0.0169, Eval R^2: -0.1276\n",
      " Epoch: 6\n",
      " Train Loss: 6.8050, Eval Loss: 12.0148\n",
      " Train R^2: 0.0299, Eval R^2: -0.1281\n",
      " Epoch: 7\n",
      " Train Loss: 6.8006, Eval Loss: 12.1867\n",
      " Train R^2: 0.0286, Eval R^2: -0.1410\n",
      " Epoch: 8\n",
      " Train Loss: 6.7405, Eval Loss: 12.0940\n",
      " Train R^2: 0.0379, Eval R^2: -0.1343\n",
      " Epoch: 9\n",
      " Train Loss: 6.7637, Eval Loss: 12.1243\n",
      " Train R^2: 0.0355, Eval R^2: -0.1360\n",
      " Epoch: 10\n",
      " Train Loss: 6.7087, Eval Loss: 12.0464\n",
      " Train R^2: 0.0418, Eval R^2: -0.1298\n",
      " Epoch: 11\n",
      " Train Loss: 6.6913, Eval Loss: 12.0649\n",
      " Train R^2: 0.0434, Eval R^2: -0.1303\n",
      " Epoch: 12\n",
      " Train Loss: 6.7144, Eval Loss: 12.0535\n",
      " Train R^2: 0.0416, Eval R^2: -0.1297\n",
      " Epoch: 13\n",
      " Train Loss: 6.6849, Eval Loss: 12.0288\n",
      " Train R^2: 0.0455, Eval R^2: -0.1281\n",
      " Epoch: 14\n",
      " Train Loss: 6.7452, Eval Loss: 12.1047\n",
      " Train R^2: 0.0368, Eval R^2: -0.1334\n",
      " Epoch: 15\n",
      " Train Loss: 6.6677, Eval Loss: 12.0738\n",
      " Train R^2: 0.0479, Eval R^2: -0.1314\n",
      " Epoch: 16\n",
      " Train Loss: 6.6461, Eval Loss: 12.0484\n",
      " Train R^2: 0.0499, Eval R^2: -0.1296\n",
      " Epoch: 17\n",
      " Train Loss: 6.6528, Eval Loss: 12.0798\n",
      " Train R^2: 0.0481, Eval R^2: -0.1317\n",
      " Epoch: 18\n",
      " Train Loss: 6.6731, Eval Loss: 12.0838\n",
      " Train R^2: 0.0467, Eval R^2: -0.1318\n",
      " Epoch: 19\n",
      " Train Loss: 6.6621, Eval Loss: 12.0834\n",
      " Train R^2: 0.0471, Eval R^2: -0.1318\n",
      "-----------------\n",
      " Year: 2015\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 8.8559, Eval Loss: 15.3975\n",
      " Train R^2: -0.2062, Eval R^2: -0.4734\n",
      " Epoch: 1\n",
      " Train Loss: 7.8204, Eval Loss: 10.6850\n",
      " Train R^2: -0.1020, Eval R^2: 0.0035\n",
      " Epoch: 2\n",
      " Train Loss: 7.3852, Eval Loss: 11.0678\n",
      " Train R^2: -0.0193, Eval R^2: -0.0308\n",
      " Epoch: 3\n",
      " Train Loss: 7.1875, Eval Loss: 12.3936\n",
      " Train R^2: 0.0023, Eval R^2: -0.1550\n",
      " Epoch: 4\n",
      " Train Loss: 7.2012, Eval Loss: 11.7122\n",
      " Train R^2: -0.0027, Eval R^2: -0.0891\n",
      " Epoch: 5\n",
      " Train Loss: 7.1571, Eval Loss: 11.6357\n",
      " Train R^2: 0.0070, Eval R^2: -0.0815\n",
      " Epoch: 6\n",
      " Train Loss: 7.1583, Eval Loss: 11.9988\n",
      " Train R^2: 0.0068, Eval R^2: -0.1146\n",
      " Epoch: 7\n",
      " Train Loss: 7.1209, Eval Loss: 11.8667\n",
      " Train R^2: 0.0107, Eval R^2: -0.1016\n",
      " Epoch: 8\n",
      " Train Loss: 7.0896, Eval Loss: 11.8051\n",
      " Train R^2: 0.0159, Eval R^2: -0.0954\n",
      " Epoch: 9\n",
      " Train Loss: 7.0940, Eval Loss: 11.8589\n",
      " Train R^2: 0.0150, Eval R^2: -0.0998\n",
      " Epoch: 10\n",
      " Train Loss: 7.0474, Eval Loss: 11.7931\n",
      " Train R^2: 0.0203, Eval R^2: -0.0932\n",
      " Epoch: 11\n",
      " Train Loss: 7.0483, Eval Loss: 11.7122\n",
      " Train R^2: 0.0208, Eval R^2: -0.0854\n",
      " Epoch: 12\n",
      " Train Loss: 7.0167, Eval Loss: 11.6860\n",
      " Train R^2: 0.0249, Eval R^2: -0.0827\n",
      " Epoch: 13\n",
      " Train Loss: 7.0090, Eval Loss: 11.6660\n",
      " Train R^2: 0.0265, Eval R^2: -0.0806\n",
      " Epoch: 14\n",
      " Train Loss: 6.9698, Eval Loss: 11.6282\n",
      " Train R^2: 0.0307, Eval R^2: -0.0769\n",
      " Epoch: 15\n",
      " Train Loss: 6.9694, Eval Loss: 11.5929\n",
      " Train R^2: 0.0317, Eval R^2: -0.0736\n",
      " Epoch: 16\n",
      " Train Loss: 6.9617, Eval Loss: 11.5576\n",
      " Train R^2: 0.0316, Eval R^2: -0.0703\n",
      " Epoch: 17\n",
      " Train Loss: 6.9836, Eval Loss: 11.5395\n",
      " Train R^2: 0.0296, Eval R^2: -0.0686\n",
      " Epoch: 18\n",
      " Train Loss: 6.9343, Eval Loss: 11.5390\n",
      " Train R^2: 0.0353, Eval R^2: -0.0685\n",
      " Epoch: 19\n",
      " Train Loss: 6.9421, Eval Loss: 11.5402\n",
      " Train R^2: 0.0349, Eval R^2: -0.0686\n",
      "-----------------\n",
      " Year: 2016\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 8.1143, Eval Loss: 9.3677\n",
      " Train R^2: -0.0542, Eval R^2: -0.0485\n",
      " Epoch: 1\n",
      " Train Loss: 7.8928, Eval Loss: 8.7521\n",
      " Train R^2: -0.0143, Eval R^2: 0.0232\n",
      " Epoch: 2\n",
      " Train Loss: 7.6291, Eval Loss: 8.5541\n",
      " Train R^2: 0.0202, Eval R^2: 0.0485\n",
      " Epoch: 3\n",
      " Train Loss: 7.4576, Eval Loss: 8.2852\n",
      " Train R^2: 0.0352, Eval R^2: 0.0685\n",
      " Epoch: 4\n",
      " Train Loss: 7.3835, Eval Loss: 8.2486\n",
      " Train R^2: 0.0434, Eval R^2: 0.0741\n",
      " Epoch: 5\n",
      " Train Loss: 7.3492, Eval Loss: 8.2329\n",
      " Train R^2: 0.0447, Eval R^2: 0.0675\n",
      " Epoch: 6\n",
      " Train Loss: 7.2543, Eval Loss: 8.2710\n",
      " Train R^2: 0.0572, Eval R^2: 0.0615\n",
      " Epoch: 7\n",
      " Train Loss: 7.1950, Eval Loss: 8.3242\n",
      " Train R^2: 0.0629, Eval R^2: 0.0501\n",
      " Epoch: 8\n",
      " Train Loss: 7.1061, Eval Loss: 8.3896\n",
      " Train R^2: 0.0740, Eval R^2: 0.0350\n",
      " Epoch: 9\n",
      " Train Loss: 7.0263, Eval Loss: 8.4687\n",
      " Train R^2: 0.0810, Eval R^2: 0.0187\n",
      " Epoch: 10\n",
      " Train Loss: 7.0277, Eval Loss: 8.5789\n",
      " Train R^2: 0.0818, Eval R^2: -0.0028\n",
      " Epoch: 11\n",
      " Train Loss: 6.9597, Eval Loss: 8.6412\n",
      " Train R^2: 0.0873, Eval R^2: -0.0165\n",
      " Epoch: 12\n",
      " Train Loss: 6.9108, Eval Loss: 8.7296\n",
      " Train R^2: 0.0927, Eval R^2: -0.0339\n",
      " Epoch: 13\n",
      " Train Loss: 6.9102, Eval Loss: 8.7610\n",
      " Train R^2: 0.0936, Eval R^2: -0.0406\n",
      " Epoch: 14\n",
      " Train Loss: 6.8888, Eval Loss: 8.8338\n",
      " Train R^2: 0.0941, Eval R^2: -0.0544\n",
      " Epoch: 15\n",
      " Train Loss: 6.8211, Eval Loss: 8.8436\n",
      " Train R^2: 0.1030, Eval R^2: -0.0572\n",
      " Epoch: 16\n",
      " Train Loss: 6.7908, Eval Loss: 8.8901\n",
      " Train R^2: 0.1063, Eval R^2: -0.0654\n",
      " Epoch: 17\n",
      " Train Loss: 6.7904, Eval Loss: 8.9273\n",
      " Train R^2: 0.1066, Eval R^2: -0.0719\n",
      " Epoch: 18\n",
      " Train Loss: 6.8007, Eval Loss: 8.9280\n",
      " Train R^2: 0.1054, Eval R^2: -0.0724\n",
      " Epoch: 19\n",
      " Train Loss: 6.7963, Eval Loss: 8.9264\n",
      " Train R^2: 0.1056, Eval R^2: -0.0722\n",
      "-----------------\n",
      " Year: 2017\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 7.9526, Eval Loss: 6.9213\n",
      " Train R^2: -0.0180, Eval R^2: 0.0128\n",
      " Epoch: 1\n",
      " Train Loss: 7.9899, Eval Loss: 6.6550\n",
      " Train R^2: -0.0193, Eval R^2: 0.0528\n",
      " Epoch: 2\n",
      " Train Loss: 7.6822, Eval Loss: 7.0096\n",
      " Train R^2: 0.0147, Eval R^2: 0.0078\n",
      " Epoch: 3\n",
      " Train Loss: 7.3893, Eval Loss: 6.0860\n",
      " Train R^2: 0.0398, Eval R^2: 0.1223\n",
      " Epoch: 4\n",
      " Train Loss: 7.3578, Eval Loss: 6.0785\n",
      " Train R^2: 0.0474, Eval R^2: 0.1239\n",
      " Epoch: 5\n",
      " Train Loss: 7.2935, Eval Loss: 5.9698\n",
      " Train R^2: 0.0529, Eval R^2: 0.1367\n",
      " Epoch: 6\n",
      " Train Loss: 7.1750, Eval Loss: 5.8036\n",
      " Train R^2: 0.0643, Eval R^2: 0.1550\n",
      " Epoch: 7\n",
      " Train Loss: 7.0922, Eval Loss: 5.6907\n",
      " Train R^2: 0.0756, Eval R^2: 0.1666\n",
      " Epoch: 8\n",
      " Train Loss: 6.9862, Eval Loss: 5.6380\n",
      " Train R^2: 0.0866, Eval R^2: 0.1710\n",
      " Epoch: 9\n",
      " Train Loss: 6.8829, Eval Loss: 5.5147\n",
      " Train R^2: 0.0987, Eval R^2: 0.1812\n",
      " Epoch: 10\n",
      " Train Loss: 6.7769, Eval Loss: 5.4629\n",
      " Train R^2: 0.1122, Eval R^2: 0.1840\n",
      " Epoch: 11\n",
      " Train Loss: 6.7090, Eval Loss: 5.4248\n",
      " Train R^2: 0.1189, Eval R^2: 0.1853\n",
      " Epoch: 12\n",
      " Train Loss: 6.6737, Eval Loss: 5.3919\n",
      " Train R^2: 0.1221, Eval R^2: 0.1846\n",
      " Epoch: 13\n",
      " Train Loss: 6.5852, Eval Loss: 5.3781\n",
      " Train R^2: 0.1337, Eval R^2: 0.1839\n",
      " Epoch: 14\n",
      " Train Loss: 6.5542, Eval Loss: 5.3746\n",
      " Train R^2: 0.1373, Eval R^2: 0.1804\n",
      " Epoch: 15\n",
      " Train Loss: 6.5099, Eval Loss: 5.3804\n",
      " Train R^2: 0.1430, Eval R^2: 0.1777\n",
      " Epoch: 16\n",
      " Train Loss: 6.4518, Eval Loss: 5.3859\n",
      " Train R^2: 0.1498, Eval R^2: 0.1754\n",
      " Epoch: 17\n",
      " Train Loss: 6.4863, Eval Loss: 5.3982\n",
      " Train R^2: 0.1451, Eval R^2: 0.1737\n",
      " Epoch: 18\n",
      " Train Loss: 6.4696, Eval Loss: 5.4015\n",
      " Train R^2: 0.1466, Eval R^2: 0.1721\n",
      " Epoch: 19\n",
      " Train Loss: 6.4612, Eval Loss: 5.4020\n",
      " Train R^2: 0.1464, Eval R^2: 0.1718\n",
      "-----------------\n",
      " Year: 2018\n",
      "-----------------\n",
      " Epoch: 0\n",
      " Train Loss: 8.4278, Eval Loss: 6.0451\n",
      " Train R^2: 0.0043, Eval R^2: 0.0313\n",
      " Epoch: 1\n",
      " Train Loss: 8.6979, Eval Loss: 6.3129\n",
      " Train R^2: -0.0110, Eval R^2: 0.0337\n",
      " Epoch: 2\n",
      " Train Loss: 8.2681, Eval Loss: 6.0436\n",
      " Train R^2: 0.0163, Eval R^2: 0.0702\n",
      " Epoch: 3\n",
      " Train Loss: 7.8354, Eval Loss: 5.5926\n",
      " Train R^2: 0.0609, Eval R^2: 0.1273\n",
      " Epoch: 4\n",
      " Train Loss: 7.7064, Eval Loss: 5.5804\n",
      " Train R^2: 0.0829, Eval R^2: 0.1310\n",
      " Epoch: 5\n",
      " Train Loss: 7.5651, Eval Loss: 5.4385\n",
      " Train R^2: 0.0891, Eval R^2: 0.1474\n",
      " Epoch: 6\n",
      " Train Loss: 7.3450, Eval Loss: 5.3308\n",
      " Train R^2: 0.1102, Eval R^2: 0.1575\n",
      " Epoch: 7\n",
      " Train Loss: 7.1547, Eval Loss: 5.2617\n",
      " Train R^2: 0.1299, Eval R^2: 0.1623\n",
      " Epoch: 8\n",
      " Train Loss: 7.0002, Eval Loss: 5.2104\n",
      " Train R^2: 0.1421, Eval R^2: 0.1649\n",
      " Epoch: 9\n",
      " Train Loss: 6.8240, Eval Loss: 5.1986\n",
      " Train R^2: 0.1618, Eval R^2: 0.1607\n",
      " Epoch: 10\n",
      " Train Loss: 6.7173, Eval Loss: 5.2012\n",
      " Train R^2: 0.1724, Eval R^2: 0.1577\n",
      " Epoch: 11\n",
      " Train Loss: 6.6319, Eval Loss: 5.2410\n",
      " Train R^2: 0.1786, Eval R^2: 0.1491\n",
      " Epoch: 12\n",
      " Train Loss: 6.5422, Eval Loss: 5.2766\n",
      " Train R^2: 0.1902, Eval R^2: 0.1435\n",
      " Epoch: 13\n",
      " Train Loss: 6.5189, Eval Loss: 5.3289\n",
      " Train R^2: 0.1921, Eval R^2: 0.1347\n",
      " Epoch: 14\n",
      " Train Loss: 6.5286, Eval Loss: 5.3743\n",
      " Train R^2: 0.1921, Eval R^2: 0.1278\n",
      " Epoch: 15\n",
      " Train Loss: 6.4447, Eval Loss: 5.4207\n",
      " Train R^2: 0.1994, Eval R^2: 0.1211\n",
      " Epoch: 16\n",
      " Train Loss: 6.4141, Eval Loss: 5.4390\n",
      " Train R^2: 0.2013, Eval R^2: 0.1182\n",
      " Epoch: 17\n",
      " Train Loss: 6.4487, Eval Loss: 5.4544\n",
      " Train R^2: 0.1987, Eval R^2: 0.1161\n",
      " Epoch: 18\n",
      " Train Loss: 6.3959, Eval Loss: 5.4711\n",
      " Train R^2: 0.2030, Eval R^2: 0.1136\n",
      " Epoch: 19\n",
      " Train Loss: 6.4144, Eval Loss: 5.4874\n",
      " Train R^2: 0.2000, Eval R^2: 0.1113\n",
      "-----------------\n",
      " End Horizon: 4\n",
      "{2014: -0.05, 2015: -0.15, 2016: -0.19, 2017: -0.11, 2018: -0.13, 2019: -0.07, 2020: -0.07, 2021: 0.17, 2022: 0.11}\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "\n",
    "for years_ahead in [4]:\n",
    "\n",
    "    print(f\"-----------------\")\n",
    "    print(f\" Horizon: {years_ahead}\")\n",
    "    print(f\"-----------------\")\n",
    "\n",
    "    start_year = 2006\n",
    "    end_year = 2022\n",
    "    train_start_year = start_year + years_ahead\n",
    "    train_end_year = end_year - years_ahead\n",
    "\n",
    "    \n",
    "    dataset_name = f\"sequence_{dataset_key}_{dataset_asset_type}_{metric_key}_{years_ahead}\"\n",
    "    model_ready_data_path = model_ready_data_paths[dataset_name]\n",
    "\n",
    "    # Load and Prepare DFs\n",
    "    df = pd.read_csv(model_ready_data_path)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df = df.sort_values(by=[\"year\"]).reset_index(drop=True)\n",
    "    df = get_most_active_municipalities(df, count_column=f\"population\", n=500)\n",
    "\n",
    "    df[\"area_code\"] = df[\"area_code\"].astype(str)\n",
    "\n",
    "    # Weighting by population\n",
    "    df[\"log_population\"] = df[\"population\"].apply(lambda x: np.log10(1 + x))\n",
    "    df[\"weight\"] = df.groupby(\"year\")[\"log_population\"].transform(lambda x: x - x.min() + 1)\n",
    "\n",
    "    all_years = list(range(start_year, end_year + 1))\n",
    "    train_years = list(range(train_start_year, train_end_year + 1))\n",
    "\n",
    "    yearly_dataframes = {\n",
    "        asset_type: {\n",
    "            f\"{year}\": drop_invalid_rows(\n",
    "                df[df[\"year\"] == year], f\"{asset_type}_{metric_sharpe}\"\n",
    "            )\n",
    "            for year in all_years\n",
    "        }\n",
    "        for asset_type in asset_types_to_train\n",
    "    }\n",
    "\n",
    "    # Finally\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # Create datasets\n",
    "    yearly_datasets = {\n",
    "        asset_type: {\n",
    "            f\"{year}\": JapanRETimeSeriesDataset(\n",
    "                df,\n",
    "                yearly_dataframes[asset_type][f\"{year}\"],\n",
    "                feature_columns=feature_columns,\n",
    "                metrics=[f\"{asset_type}_{normalized_metric_sharpe}\"],\n",
    "                weight_column=f\"weight\",\n",
    "                transform=transforms.Compose([ToNumpy(), PadAndMask(), ToTensor()]),\n",
    "                shift=years_ahead,\n",
    "            )\n",
    "            for year in all_years\n",
    "        }\n",
    "        for asset_type in asset_types_to_train\n",
    "    }\n",
    "\n",
    "    # Train\n",
    "\n",
    "    # Incremental training and evaluation\n",
    "    progress_bar = None\n",
    "    save_predictions = True\n",
    "\n",
    "    train_losses, train_r2_scores = [], []\n",
    "    eval_losses, eval_r2_scores = [], []\n",
    "\n",
    "    final_train_losses, final_train_r2_scores = {}, {}\n",
    "    final_eval_losses, final_eval_r2_scores = {}, {}\n",
    "\n",
    "    for year in train_years:\n",
    "        print(f\"-----------------\")\n",
    "        print(f\" Year: {year}\")\n",
    "        print(f\"-----------------\")\n",
    "\n",
    "        # Compile dataset\n",
    "        train_dataset = ConcatDataset(\n",
    "            [\n",
    "                yearly_datasets[asset_type][f\"{train_year}\"]\n",
    "                for train_year in range(train_start_year, year + 1)\n",
    "                for asset_type in asset_types_to_train\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "        eval_dataset = ConcatDataset(\n",
    "            [\n",
    "                yearly_datasets[asset_type][f\"{year + years_ahead}\"]\n",
    "                for asset_type in asset_types_to_train\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        eval_dataloader = DataLoader(\n",
    "            eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "        # Create new model\n",
    "        model = TimeSeriesTransformerModel(\n",
    "            n_features=n_features,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            d_hid=d_hid,\n",
    "            nlayers=nlayers,\n",
    "            dropout=dropout,\n",
    "            enc_dropout=enc_dropout,\n",
    "            device=device,\n",
    "        )\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Prepare training params\n",
    "        num_training_steps = num_epochs_per_year * len(train_dataloader)\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",  # constant\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(num_epochs_per_year):\n",
    "            train_loss, train_r2_score = train_weighted(\n",
    "                model,\n",
    "                train_dataloader,\n",
    "                optimizer,\n",
    "                lr_scheduler,\n",
    "                progress_bar,\n",
    "                device=device,\n",
    "            )\n",
    "            train_losses.append(train_loss)\n",
    "            train_r2_scores.append(train_r2_score)\n",
    "\n",
    "            eval_loss, eval_r2_score = evaluate_weighted(\n",
    "                model, eval_dataloader, device=device\n",
    "            )\n",
    "            eval_losses.append(eval_loss)\n",
    "            eval_r2_scores.append(eval_r2_score)\n",
    "\n",
    "            print(f\" Epoch: {epoch}\")\n",
    "            print(f\" Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
    "            print(f\" Train R^2: {train_r2_score:.4f}, Eval R^2: {eval_r2_score:.4f}\")\n",
    "\n",
    "        # Save year end results\n",
    "        final_train_losses[year] = train_loss\n",
    "        final_train_r2_scores[year] = train_r2_score\n",
    "        final_eval_losses[year + years_ahead] = eval_loss\n",
    "        final_eval_r2_scores[year + years_ahead] = eval_r2_score\n",
    "\n",
    "        if save_predictions:\n",
    "            for investment_asset_type in asset_types_to_train:\n",
    "\n",
    "                dataset_name = f\"sequence_{dataset_key}_{investment_asset_type}_{metric_key}_{years_ahead}\"\n",
    "                output_dataset_name = f\"{dataset_name}_{year + years_ahead}\"\n",
    "                model_output_data_path = model_output_data_paths[output_dataset_name]\n",
    "\n",
    "                prediction_df = yearly_dataframes[investment_asset_type][f\"{year + years_ahead}\"]\n",
    "\n",
    "                prediction_df[\"predicted_normalized_return\"] = predict_returns(\n",
    "                    model,\n",
    "                    df,\n",
    "                    prediction_df,\n",
    "                    investment_asset_type,\n",
    "                    feature_columns,\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "                prediction_df[\"asset_type\"] = investment_asset_type\n",
    "                prediction_df[\n",
    "                    [\n",
    "                        \"year\",\n",
    "                        \"area_code\",\n",
    "                        \"asset_type\",\n",
    "                        \"predicted_normalized_return\",\n",
    "                        f\"{investment_asset_type}_yearly_price_growth\",\n",
    "                        f\"{investment_asset_type}_{metric_sharpe}\",\n",
    "                        f\"{investment_asset_type}_{normalized_metric_sharpe}\",\n",
    "                    ]\n",
    "                ].to_csv(model_output_data_path, index=False)\n",
    "\n",
    "    print(f\"-----------------\")\n",
    "    print(f\" End Horizon: {years_ahead}\")\n",
    "    print({key: round(value, 2) for key, value in final_eval_r2_scores.items()})\n",
    "    print(f\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    2008: -0.06,\n",
    "    2009: -0.01,\n",
    "    2010: 0.02,\n",
    "    2011: 0.14,\n",
    "    2012: 0.22,\n",
    "    2013: 0.27,\n",
    "    2014: 0.25,\n",
    "    2015: 0.31,\n",
    "    2016: 0.18,\n",
    "    2017: 0.23,\n",
    "    2018: 0.31,\n",
    "    2019: 0.29,\n",
    "    2020: 0.32,\n",
    "    2021: 0.26,\n",
    "    2022: 0.33,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
